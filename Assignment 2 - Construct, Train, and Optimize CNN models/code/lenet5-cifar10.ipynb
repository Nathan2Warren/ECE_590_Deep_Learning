{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training LeNet-5 on CIFAR-10\n",
    "In this notebook, we will try to deploy the famous LeNet-5 to solve a simple image classification task, the CIFAR-10. CIFAR-10 is composed of 60K images from 10 categories. After splitting the dataset, we have 45K/5K/10K images for train/valiation/test dataset.\n",
    "In this notebook, only the labels of training/validation dataset is visible to you, so you can use the training and validation data to tune your model. After you submitted your model, your final grade will be determined on the model performance on the holdout test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0 Setting up LeNet-5 model\n",
    "As you have set up the LeNet-5 model in Homework 1, we will just move the implementation of LeNet-5 model here, so you can use it for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Import pytorch dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Create the neural network module: LeNet-5\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "        #self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode = 'fan_in')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                nn.init.kaiming_normal_(m.weight, mode = 'fan_in')\n",
    "                #m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setting up preprocessing functions.\n",
    "Preprocessing is very important because it prepares your data for proceeding training steps.\n",
    "Write functions to load dataset and preprocess the incoming data. We recommend that the preprocess scheme \\textbf{must} include normalize, standardization, batch shuffling to make sure the training \n",
    "process goes smoothly. The preprocess scheme may also contain some data augmentation methods \n",
    "(e.g., random crop, random flip, etc.). \n",
    "\n",
    "Reference value for mean/std:\n",
    "\n",
    "**mean(RGB-format): (0.4914, 0.4822, 0.4465)**\n",
    "\n",
    "**std(RGB-format): (0.2023, 0.1994, 0.2010)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify preprocessing function.\n",
    "# Reference mean/std value for \n",
    "transform_train  = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Setting up data I/O\n",
    "Data I/O reads data from the dataset and prepares it for further procedures. Note that you have to link transformation with data I/O so that these operations can be interleaved. Thus, the training process can be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/cifar10_trainval_F20.zip\n",
      "Extracting ./data/cifar10_trainval_F20.zip to ./data\n",
      "Files already downloaded and verified\n",
      "Training dataset has 45000 examples!\n",
      "Using downloaded and verified file: ./data/cifar10_trainval_F20.zip\n",
      "Extracting ./data/cifar10_trainval_F20.zip to ./data\n",
      "Files already downloaded and verified\n",
      "Validation dataset has 5000 examples!\n"
     ]
    }
   ],
   "source": [
    "# You cannot change this line.\n",
    "from tools.dataloader import CIFAR10\n",
    "# Call the dataset Loader\n",
    "DATAROOT = \"./data\"\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 100\n",
    "trainset = CIFAR10(root=DATAROOT, train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "valset = CIFAR10(root=DATAROOT, train=False, download=True, transform=transform_val)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Instantialize your LeNet-5 model and deploy it to GPU devices.\n",
    "You may want to deploy your model to GPU device for efficient training. Please assign your model to GPU if possible. If you are training on a machine without GPUs, please deploy your model to CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on GPU...\n"
     ]
    }
   ],
   "source": [
    "# Specify the device for computation\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = LeNet5()\n",
    "net = net.to(device)\n",
    "if device =='cuda':\n",
    "    print(\"Train on GPU...\")\n",
    "else:\n",
    "    print(\"Train on CPU...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter settings\n",
    "Hyperparameters are quite crucial in determining the performance of our model. The default hyperparameter settings are sufficient for a decent result. You may tune them wisely and carefully for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial learning rate\n",
    "INITIAL_LR = 0.01\n",
    "# Momentum for optimizer.\n",
    "MOMENTUM = 0.9\n",
    "# Regularization\n",
    "REG = 1e-4\n",
    "# Total number of training epochs\n",
    "EPOCHS = 30\n",
    "# Learning rate decay policy.\n",
    "DECAY_EPOCHS = 2\n",
    "DECAY = 1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling weights load/save protocols.\n",
    "This handles the weight loading/saving protocols.You may be able to load from checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training from scratch ...\n",
      "Starting from learning rate 0.010000:\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH = \"./saved_model\"\n",
    "# FLAG for loading the pretrained model\n",
    "TRAIN_FROM_SCRATCH = True\n",
    "# Code for loading checkpoint and recover epoch id.\n",
    "CKPT_PATH = \"./saved_model/model.h5\"\n",
    "def get_checkpoint(ckpt_path):\n",
    "    try:\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    return ckpt\n",
    "\n",
    "ckpt = get_checkpoint(CKPT_PATH)\n",
    "if ckpt is None or TRAIN_FROM_SCRATCH:\n",
    "    if not TRAIN_FROM_SCRATCH:\n",
    "        print(\"Checkpoint not found.\")\n",
    "    print(\"Training from scratch ...\")\n",
    "    start_epoch = 0\n",
    "    current_learning_rate = INITIAL_LR\n",
    "else:\n",
    "    print(\"Successfully loaded checkpoint: %s\" %CKPT_PATH)\n",
    "    net.load_state_dict(ckpt['net'])\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    current_learning_rate = ckpt['lr']\n",
    "    print(\"Starting from epoch %d \" %start_epoch)\n",
    "\n",
    "print(\"Starting from learning rate %f:\" %current_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Setting up loss functions and Optimizers\n",
    "Loss function is your objective to train the neural networks. Typically, we use multi-class cross entropy as objectives for classification models (e.g., CIFAR-10, MNIST). In this homework, we use SGD optimizer with momentum as our optimizer. You need to formulate the cross-entropy loss function in PyTorch.\n",
    "You should also specify a PyTorch Optimizer to optimize this loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss function and specify regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Add optimizer\n",
    "optimizer = optim.SGD(params=net.parameters(), lr=current_learning_rate, momentum=MOMENTUM, weight_decay=REG, nesterov=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Start the training process.\n",
    "Congratulations! You have completed all of the previous steps and it is time to train our neural network.\n",
    "Training a neural network usually composes the following 3 parts: \n",
    "\n",
    "**i) Get a batch of data from the dataloader and copy it to your device (GPU)**\n",
    "\n",
    "**ii) Do a forward pass to get the output logits from the neural network. Compute the forward loss.**\n",
    "\n",
    "**iii) Do a backward pass (back-propagation) to compute gradients of all weights with respect to the loss.**\n",
    "\n",
    "You will also need to compute accuracy within all these parts to justify that your model is doing well on the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-20 23:01:17.986492\n",
      "Epoch 0:\n",
      "loss: 2.2965221405029297\n",
      "Training loss: 1.9072, Training accuracy: 0.2969\n",
      "2020-09-20 23:01:26.514175\n",
      "Validation...\n",
      "Validation loss: 1.5809, Validation accuracy: 0.4296\n",
      "\n",
      "Saving ...\n",
      "\n",
      "2020-09-20 23:01:27.100458\n",
      "Epoch 1:\n",
      "loss: 1.5504249334335327\n",
      "Training loss: 1.4904, Training accuracy: 0.4591\n",
      "2020-09-20 23:01:34.692813\n",
      "Validation...\n",
      "Validation loss: 1.3662, Validation accuracy: 0.5168\n",
      "\n",
      "Saving ...\n",
      "\n",
      "2020-09-20 23:01:35.407331\n",
      "Epoch 2:\n",
      "loss: 1.25675630569458\n",
      "Training loss: 1.3264, Training accuracy: 0.5253\n",
      "2020-09-20 23:01:41.955890\n",
      "Validation...\n",
      "Validation loss: 1.3298, Validation accuracy: 0.5272\n",
      "\n",
      "Current learning rate has decayed to 0.010000\n",
      "Saving ...\n",
      "\n",
      "2020-09-20 23:01:42.809872\n",
      "Epoch 3:\n",
      "loss: 1.3207398653030396\n",
      "Training loss: 1.2240, Training accuracy: 0.5656\n",
      "2020-09-20 23:01:49.598877\n",
      "Validation...\n",
      "Validation loss: 1.2189, Validation accuracy: 0.5652\n",
      "\n",
      "Saving ...\n",
      "\n",
      "2020-09-20 23:01:50.261719\n",
      "Epoch 4:\n",
      "loss: 0.9976664781570435\n",
      "Training loss: 1.1336, Training accuracy: 0.5984\n",
      "2020-09-20 23:01:56.304082\n",
      "Validation...\n",
      "Validation loss: 1.1388, Validation accuracy: 0.6008\n",
      "\n",
      "Current learning rate has decayed to 0.010000\n",
      "Saving ...\n",
      "\n",
      "2020-09-20 23:01:56.986013\n",
      "Epoch 5:\n",
      "loss: 1.1137473583221436\n",
      "Training loss: 1.0717, Training accuracy: 0.6221\n",
      "2020-09-20 23:02:03.251035\n",
      "Validation...\n",
      "Validation loss: 1.1427, Validation accuracy: 0.5968\n",
      "\n",
      "2020-09-20 23:02:03.927011\n",
      "Epoch 6:\n",
      "loss: 1.124541163444519\n",
      "Training loss: 1.0083, Training accuracy: 0.6420\n",
      "2020-09-20 23:02:10.698507\n",
      "Validation...\n",
      "Validation loss: 1.1323, Validation accuracy: 0.6050\n",
      "\n",
      "Current learning rate has decayed to 0.010000\n",
      "Saving ...\n",
      "\n",
      "2020-09-20 23:02:11.315663\n",
      "Epoch 7:\n",
      "loss: 1.079245686531067\n",
      "Training loss: 0.9700, Training accuracy: 0.6560\n",
      "2020-09-20 23:02:18.605063\n",
      "Validation...\n",
      "Validation loss: 1.1390, Validation accuracy: 0.6158\n",
      "\n",
      "Saving ...\n",
      "\n",
      "2020-09-20 23:02:19.339381\n",
      "Epoch 8:\n",
      "loss: 0.8523988127708435\n",
      "Training loss: 0.9207, Training accuracy: 0.6747\n",
      "2020-09-20 23:02:26.209077\n",
      "Validation...\n",
      "Validation loss: 1.0826, Validation accuracy: 0.6264\n",
      "\n",
      "Current learning rate has decayed to 0.010000\n",
      "Saving ...\n",
      "\n",
      "2020-09-20 23:02:26.884616\n",
      "Epoch 9:\n",
      "loss: 0.8664131164550781\n",
      "Training loss: 0.8790, Training accuracy: 0.6898\n",
      "2020-09-20 23:02:33.423019\n",
      "Validation...\n",
      "Validation loss: 1.0751, Validation accuracy: 0.6306\n",
      "\n",
      "Saving ...\n",
      "\n",
      "2020-09-20 23:02:34.171515\n",
      "Epoch 10:\n",
      "loss: 0.7657893896102905\n",
      "Training loss: 0.8461, Training accuracy: 0.6998\n",
      "2020-09-20 23:02:41.048651\n",
      "Validation...\n",
      "Validation loss: 1.0686, Validation accuracy: 0.6406\n",
      "\n",
      "Current learning rate has decayed to 0.010000\n",
      "Saving ...\n",
      "\n",
      "2020-09-20 23:02:41.752653\n",
      "Epoch 11:\n",
      "loss: 0.7276546955108643\n",
      "Training loss: 0.8146, Training accuracy: 0.7098\n",
      "2020-09-20 23:02:48.366964\n",
      "Validation...\n",
      "Validation loss: 1.1219, Validation accuracy: 0.6144\n",
      "\n",
      "2020-09-20 23:02:49.019051\n",
      "Epoch 12:\n",
      "loss: 0.7574480772018433\n",
      "Training loss: 0.7770, Training accuracy: 0.7236\n",
      "2020-09-20 23:02:55.781321\n",
      "Validation...\n",
      "Validation loss: 1.1379, Validation accuracy: 0.6320\n",
      "\n",
      "Current learning rate has decayed to 0.010000\n",
      "2020-09-20 23:02:56.483377\n",
      "Epoch 13:\n",
      "loss: 0.7035405039787292\n",
      "Training loss: 0.7428, Training accuracy: 0.7365\n",
      "2020-09-20 23:03:03.674908\n",
      "Validation...\n",
      "Validation loss: 1.1014, Validation accuracy: 0.6354\n",
      "\n",
      "2020-09-20 23:03:04.344456\n",
      "Epoch 14:\n",
      "loss: 0.6284312605857849\n",
      "Training loss: 0.7156, Training accuracy: 0.7450\n",
      "2020-09-20 23:03:10.254990\n",
      "Validation...\n",
      "Validation loss: 1.1754, Validation accuracy: 0.6276\n",
      "\n",
      "Current learning rate has decayed to 0.010000\n",
      "2020-09-20 23:03:10.972625\n",
      "Epoch 15:\n",
      "loss: 0.7252336740493774\n",
      "Training loss: 0.6840, Training accuracy: 0.7559\n",
      "2020-09-20 23:03:17.812055\n",
      "Validation...\n",
      "Validation loss: 1.1322, Validation accuracy: 0.6342\n",
      "\n",
      "2020-09-20 23:03:18.552722\n",
      "Epoch 16:\n",
      "loss: 0.762003481388092\n",
      "Training loss: 0.6619, Training accuracy: 0.7638\n",
      "2020-09-20 23:03:24.888760\n",
      "Validation...\n",
      "Validation loss: 1.1769, Validation accuracy: 0.6284\n",
      "\n",
      "Current learning rate has decayed to 0.010000\n",
      "2020-09-20 23:03:25.632966\n",
      "Epoch 17:\n",
      "loss: 0.4577125012874603\n",
      "Training loss: 0.6356, Training accuracy: 0.7719\n",
      "2020-09-20 23:03:31.743285\n",
      "Validation...\n",
      "Validation loss: 1.2024, Validation accuracy: 0.6262\n",
      "\n",
      "2020-09-20 23:03:32.446129\n",
      "Epoch 18:\n",
      "loss: 0.6626521348953247\n",
      "Training loss: 0.6205, Training accuracy: 0.7775\n",
      "2020-09-20 23:03:38.883051\n",
      "Validation...\n",
      "Validation loss: 1.2499, Validation accuracy: 0.6250\n",
      "\n",
      "Current learning rate has decayed to 0.010000\n",
      "2020-09-20 23:03:39.571071\n",
      "Epoch 19:\n",
      "loss: 0.5983189940452576\n",
      "Training loss: 0.5987, Training accuracy: 0.7837\n",
      "2020-09-20 23:03:46.173812\n",
      "Validation...\n",
      "Validation loss: 1.2967, Validation accuracy: 0.6260\n",
      "\n",
      "2020-09-20 23:03:46.967986\n",
      "Epoch 20:\n",
      "loss: 0.428577184677124\n",
      "Training loss: 0.5820, Training accuracy: 0.7926\n",
      "2020-09-20 23:03:54.216856\n",
      "Validation...\n",
      "Validation loss: 1.3033, Validation accuracy: 0.6158\n",
      "\n",
      "Current learning rate has decayed to 0.010000\n",
      "2020-09-20 23:03:54.928502\n",
      "Epoch 21:\n",
      "loss: 0.46003782749176025\n",
      "Training loss: 0.5607, Training accuracy: 0.7989\n",
      "2020-09-20 23:04:02.411025\n",
      "Validation...\n",
      "Validation loss: 1.3585, Validation accuracy: 0.6208\n",
      "\n",
      "2020-09-20 23:04:03.071423\n",
      "Epoch 22:\n",
      "loss: 0.5283034443855286\n",
      "Training loss: 0.5433, Training accuracy: 0.8047\n",
      "2020-09-20 23:04:10.774612\n",
      "Validation...\n",
      "Validation loss: 1.3329, Validation accuracy: 0.6280\n",
      "\n",
      "Current learning rate has decayed to 0.010000\n",
      "2020-09-20 23:04:11.599144\n",
      "Epoch 23:\n",
      "loss: 0.38734596967697144\n",
      "Training loss: 0.5205, Training accuracy: 0.8107\n",
      "2020-09-20 23:04:18.257282\n",
      "Validation...\n",
      "Validation loss: 1.3805, Validation accuracy: 0.6278\n",
      "\n",
      "2020-09-20 23:04:19.020402\n",
      "Epoch 24:\n",
      "loss: 0.4666733145713806\n",
      "Training loss: 0.5076, Training accuracy: 0.8178\n",
      "2020-09-20 23:04:25.648711\n",
      "Validation...\n",
      "Validation loss: 1.4057, Validation accuracy: 0.6180\n",
      "\n",
      "Current learning rate has decayed to 0.010000\n",
      "2020-09-20 23:04:26.371586\n",
      "Epoch 25:\n",
      "loss: 0.3964575529098511\n",
      "Training loss: 0.4963, Training accuracy: 0.8211\n",
      "2020-09-20 23:04:33.447148\n",
      "Validation...\n",
      "Validation loss: 1.4587, Validation accuracy: 0.6172\n",
      "\n",
      "2020-09-20 23:04:34.247097\n",
      "Epoch 26:\n",
      "loss: 0.37982362508773804\n",
      "Training loss: 0.4733, Training accuracy: 0.8312\n",
      "2020-09-20 23:04:41.266521\n",
      "Validation...\n",
      "Validation loss: 1.4917, Validation accuracy: 0.6214\n",
      "\n",
      "Current learning rate has decayed to 0.010000\n",
      "2020-09-20 23:04:41.947440\n",
      "Epoch 27:\n",
      "loss: 0.4592314660549164\n",
      "Training loss: 0.4578, Training accuracy: 0.8342\n",
      "2020-09-20 23:04:48.815112\n",
      "Validation...\n",
      "Validation loss: 1.5700, Validation accuracy: 0.6158\n",
      "\n",
      "2020-09-20 23:04:49.590747\n",
      "Epoch 28:\n",
      "loss: 0.5798604488372803\n",
      "Training loss: 0.4597, Training accuracy: 0.8338\n",
      "2020-09-20 23:04:56.823107\n",
      "Validation...\n",
      "Validation loss: 1.6303, Validation accuracy: 0.6096\n",
      "\n",
      "Current learning rate has decayed to 0.010000\n",
      "2020-09-20 23:04:57.579155\n",
      "Epoch 29:\n",
      "loss: 0.35680726170539856\n",
      "Training loss: 0.4418, Training accuracy: 0.8420\n",
      "2020-09-20 23:05:04.142428\n",
      "Validation...\n",
      "Validation loss: 1.6914, Validation accuracy: 0.6026\n",
      "\n",
      "Optimization finished.\n"
     ]
    }
   ],
   "source": [
    "# Start the training/validation process\n",
    "# The process should take about 5 minutes on a GTX 1070-Ti\n",
    "# if the code is written efficiently.\n",
    "global_step = 0\n",
    "best_val_acc = 0\n",
    "\n",
    "train_acc_l, val_acc_l = [],[]\n",
    "\n",
    "for i in range(start_epoch, EPOCHS):\n",
    "    print(datetime.datetime.now())\n",
    "    # Switch to train mode\n",
    "    net.train()\n",
    "    print(\"Epoch %d:\" %i)\n",
    "\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    \n",
    "    # Train the training dataset for 1 epoch.\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # Copy inputs to device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Generate output\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        # Now backward loss\n",
    "        loss.backward()\n",
    "        # Print initial loss\n",
    "        if batch_idx == 0:\n",
    "            print(f'loss: {loss}')\n",
    "        # Apply gradient\n",
    "        optimizer.step()\n",
    "        # Calculate predicted labels\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_examples += predicted.size(0)\n",
    "        correct_examples += predicted.eq(targets).sum().item()\n",
    "        train_loss += loss\n",
    "        global_step += 1\n",
    "                \n",
    "    avg_loss = train_loss / (batch_idx + 1)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
    "    train_acc_l.append(avg_acc)\n",
    "    print(datetime.datetime.now())\n",
    "    # Validate on the validation dataset\n",
    "    print(\"Validation...\")\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    # Disable gradient during validation\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            # Copy inputs to device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # Zero the gradient\n",
    "            optimizer.zero_grad()\n",
    "            # Generate output from the DNN.\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)            \n",
    "            # Calculate predicted labels\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_examples += predicted.size(0)\n",
    "            correct_examples += predicted.eq(targets).sum().item()\n",
    "            val_loss += loss\n",
    "\n",
    "    avg_loss = val_loss / len(valloader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    val_acc_l.append(avg_acc)\n",
    "    \n",
    "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\\n\" % (avg_loss, avg_acc))\n",
    "\n",
    "    # Handle the learning rate scheduler.\n",
    "    if i % DECAY_EPOCHS == 0 and i != 0:\n",
    "        current_learning_rate = current_learning_rate * DECAY\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = current_learning_rate\n",
    "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
    "    \n",
    "    # Save for checkpoint\n",
    "    if avg_acc > best_val_acc:\n",
    "        best_val_acc = avg_acc\n",
    "        if not os.path.exists(CHECKPOINT_PATH):\n",
    "            os.makedirs(CHECKPOINT_PATH)\n",
    "        print(\"Saving ...\\n\")\n",
    "        state = {'net': net.state_dict(),\n",
    "                 'epoch': i,\n",
    "                 'lr': current_learning_rate}\n",
    "        torch.save(state, os.path.join(CHECKPOINT_PATH, 'model.h5'))\n",
    "\n",
    "print(\"Optimization finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.DataFrame(list(zip(train_acc_l, val_acc_l)), columns = ['Train Accuracy LR1', 'Val Accuracy LR1'])\n",
    "df.to_csv('bn_lr10NEWEST.csv')\n",
    "\n",
    "#df = pd.DataFrame(list(zip(train_acc_l, val_acc_l)), columns = ['Train Accuracy', 'Val Accuracy'])\n",
    "#df.to_csv('Uniform Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kaiming = pd.read_csv(\"Kaiming Accuracy.csv\")\n",
    "#default = pd.read_csv('Uniform Accuracy.csv')\n",
    "#df = pd.concat([default, kaiming], axis=1, ignore_index=True)\n",
    "#df = df.drop(3, axis = 1)\n",
    "#df.columns = ['Batch Number', 'Train Acc', ' Val Acc', 'Kaiming Train Acc', 'Kaiming Val Acc']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
